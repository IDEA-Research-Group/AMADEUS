
from bs4 import BeautifulSoup
from urllib.request import Request, urlopen, HTTPRedirectHandler
import urllib.parse
import requests
import json
import time

from .redis_store import *

from scrapping.structures import Exploit

class ExploitDbScraper():

    CVE_SEARCH_ENDPOINT = "https://www.exploit-db.com/?draw=1&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=true&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start=0&length=200&search%5Bvalue%5D={}&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_=1606594141644"
    
    def __init__(self):
        pass

    def get_exploits_for_CVE(self, CVEid : str):

        # TODO Paging
        if not CVEid or not type(CVEid) == str:
            raise TypeError("CVE id must be a valid string")

        cachedResult = get_exploits_from_cve(CVEid)
        if cachedResult != None:
            return cachedResult

        while True:
            try:
                resp = requests.get(ExploitDbScraper.CVE_SEARCH_ENDPOINT.format(CVEid.replace('CVE-','')), headers={"x-requested-with": "XMLHttpRequest"})
                break
            except ConnectionError:
                print("[WARN] Connection error getting exploits. Retrying")
                time.sleep(5)
        

        jsonObject = json.loads(resp.text)
        data = jsonObject["data"]
        exploits = []
        for e in data:
            exploits.append(Exploit(e))

        store_exploits_from_cve(CVEid, exploits)
        return exploits
    
    def get_exploits_for_CPE(self, cpe: str, scraper, excludeCVE, cveExploitDict):
        '''
        Returns exploits that affect the given cpe configuration using the given VulnerabilityScraper
        '''
        if not cpe or type(cpe) is not str:
            raise TypeError("CPE must be a valid string")
        if not scraper:
            raise TypeError("A valid VulnerabilityScraper object must be provided")

        cves = scraper.get_CVEs_from_CPE(cpe)
        
        exploits = []
        for cve in cves:
            if excludeCVE and cve.cve_id == excludeCVE.cve_id:
                continue
            if cve.cve_id in cveExploitDict:
                for ex in cveExploitDict[cve.cve_id]:
                    exploits.append(ex)
            else:
                l = scraper.get_exploits_for_CVE(cve)
                cveExploitDict[cve.cve_id] = l
                for e in l:
                    exploits.append(e)

        return exploits